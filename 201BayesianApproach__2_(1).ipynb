{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adinsa1/Data110/blob/main/201BayesianApproach__2_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArKnD5zdLPOM"
      },
      "source": [
        "**Reverend Thomas Bayes**\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWT_-JhQuVGf"
      },
      "source": [
        "## Bayes Theorem\n",
        "\n",
        "The bayes theorem can be easily derived from conditional probability and is shown as the last formula below.\n",
        "\n",
        "<img src=\"https://sites.google.com/site/artificialcortext/_/rsrc/1270096677974/others/mathematics/bayes-theorem/bayes-der.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhzATxg4qZf5"
      },
      "source": [
        "### Example\n",
        "\n",
        "Let us roll a fair die:\n",
        "\n",
        "B = event of getting an odd number\n",
        "\n",
        "A= event of getting 1\n",
        "\n",
        "P(A|B) = P(B|A)*P(A) / P(B)\n",
        "\n",
        "P(B|A) = 1, since the probability of getting an odd number is certain, given that we got 1\n",
        "\n",
        "P(A) = 1/6 and P(B) = 1/2 (half of the numbers on the die are odd). Plugging in the numbers into Bayes' formula gives us: P(A|B) = 1/3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ8U3SS7HbnH"
      },
      "source": [
        "\n",
        "<img src = \"https://i.pinimg.com/originals/5d/90/79/5d9079798db012ca2af8806d7d211fdf.png\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLayJoWXvuOp"
      },
      "source": [
        "## Fequentist\n",
        "\n",
        "There are different definitions of what we mean by a probability.\n",
        "\n",
        "Frequentists define probability as the proportion of times (frequency) an event occurs if the trials are repeated many times. This can apply to tossing a coin. But there are events that we cannot try many times, e.g., what is the probability that humans caused global warming?\n",
        "\n",
        "A population parameter, e.g., the average height of the population is fixed although we don't know it. There is no uncertainty or probability of the parameter. Similarly, a hypothesis is either true or false; no probability of a hypothesis being true.\n",
        "\n",
        "Frequentists use the maximum of the likelihoods P(D|H) to choose a hypothesis. What you have learned so far is frequentist statistics (classical statistics):\n",
        "P-values, confidence intervals, Null hypothesis testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8YWGLyMIOmI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNRnhqjZvyK2"
      },
      "source": [
        "## Bayesian\n",
        "Bayesians define probabilty as a quantification of a personal belief. They assign a probability to a hypothesis or to a population parameter or to events that are not repeatable or events that happened in the past.\n",
        "\n",
        "The probability of a hypothesis is updated as soon as new evidence (data) comes in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT8g2L-PPsYL"
      },
      "source": [
        "<img src=\"https://1.bp.blogspot.com/-bjvuMHBr_60/XVQXnNfw7NI/AAAAAAAAPzw/TCQGzgDMoNQka7KZAVbMjLQMhmFdza4zgCLcBGAs/w1200-h630-p-k-no-nu/Screenshot%2B2019-08-14%2B10.15.52.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc0Bwmj2R2mm"
      },
      "source": [
        "## Example\n",
        "\n",
        "A medical lab test results positive for 99% of all sick people,i.e., P(test positive | sick person) = 0.99\n",
        "\n",
        "The lab test results negative for 98% of all healthy people, i.e., P(test negative | healthy person) = 0.98\n",
        "\n",
        "The disease prevalence in the population is 1 in 1000 persons, i.e., P(sick person) = 0.001\n",
        "\n",
        "If someone gets a positive result, what is the propability that the person has the disease?\n",
        "\n",
        "Many docotrs overestimate this probability:\n",
        "\n",
        "https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/1861033\n",
        "\n",
        "https://www.washingtonpost.com/news/posteverything/wp/2018/10/05/feature/doctors-are-surprisingly-bad-at-reading-lab-results-its-putting-us-all-at-risk/?noredirect=on&utm_term=.51f87fcf8e25\n",
        "\n",
        "\n",
        "Let us use Bayes formula to answer the question:\n",
        "\n",
        "P(person sick | test positive) = P(test positive| person sick)*P(person sick)/ p(test positive)\n",
        "\n",
        "Let us work on the denominator:\n",
        "\n",
        "P(test positive) = P(test positive | person sick) * P(person sick) + P(test positive | person healthy)*P(person healthy)\n",
        "\n",
        "Putting together:\n",
        "\n",
        "P(person sick | test positive) = (0.99 * 0.001) / (0.99*0.001 + 0.02*0.999) = 0.047 or 4.7%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RipdzmleTax"
      },
      "source": [
        "### Three competing hypotheses:\n",
        "\n",
        "A friend has three coins: The first coin has heads and tails with equal probability, the second one has two heads and the third one has two tails.  He chose one of the coins randomly and tossed it. It landed heads up. What is the probability that he chose the first or the second or the third coin?\n",
        "\n",
        "We have competing hypotheses (explanations) as to which coin he chose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZSgnbf-vZKd"
      },
      "source": [
        "### Prior P(H)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkFokyZsb2M0"
      },
      "source": [
        "import numpy as np\n",
        "# 0 ->fair coin HT\n",
        "# 1-> two heads coin HH\n",
        "# 2 -> two tils coin TT\n",
        "prior = np.array([1/3,1/3,1/3])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtaE3b_qve7V"
      },
      "source": [
        "### Likelihood P(D|H)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvnbaqHevk0M"
      },
      "source": [
        "# Likelihood for heads: P(heads | first coin) = 1/2; P(heads | second coin) = 1; P(heads|third coin) = 0\n",
        "likelihood_h = np.array([1/2,1,0])\n",
        "\n",
        "# Likelihood for tails\n",
        "likelihood_t = np.array([1/2,0,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dV2yT1vph9"
      },
      "source": [
        "### Posterior P(H|D) = likelihood * Prior / P(D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOIV_KSfGRb",
        "outputId": "739b200b-c1c4-41a9-a995-4c7a3ae778a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We observe heads H:\n",
        "# We update our probability for each hypothesis in light of the new evidence. Change your mind !\n",
        "posterior = list([1/3,1/3,1/3])\n",
        "for i in range(3):\n",
        "    posterior[i] = likelihood_h[i] * prior[i]  / np.sum(likelihood_h * prior)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3333333333333333, 0.6666666666666666, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOOh7ytpsrrq"
      },
      "source": [
        "The probability of the first coin didn't change. The probability of the second coin doubled. The probability of the third coin changed to zero. If we observe heads, it can be the two tails coin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KKtleBjgaF6",
        "outputId": "6d6ba2e0-c896-46a1-a2d9-88f05dc318c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We observe again heads H. The previous posterior is the new prior. Again we change our minds!\n",
        "prior = posterior\n",
        "for i in range(3):\n",
        "    posterior[i] = likelihood_h[i] * prior[i]  / np.sum(likelihood_h * prior)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2, 0.8695652173913043, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPtiKfAltwRg"
      },
      "source": [
        "The probability of the first coin decreases, that of the second coin increases, and third coin stays zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHnupRamCYk",
        "outputId": "618ef7ae-4c4f-49b1-e091-3f3c34a3aeb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We observe again heads H. The previous posterior is the new prior\n",
        "prior = posterior\n",
        "for i in range(3):\n",
        "    posterior[i] = likelihood_h[i] * prior[i]  / np.sum(likelihood_h * prior)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1031390134529148, 0.9440152397079056, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQUKTXwuJWv"
      },
      "source": [
        "Now it looks like the first coin is improbable (p=0.103) and the second coin is almost certain (p=0.944). It is certainly not the third coin (p=0).\n",
        "\n",
        "What happens if the friend tosses the coin one more time and it lands tails up?\n",
        "\n",
        "Here is the wisdom of the detective Sherlock Holmes:\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/8b/12/29/8b122907f6ddafbf1aea70d7ee6076e0.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l51z3TvrmGCU",
        "outputId": "3451ed66-5176-4757-fd2a-3dd65a89887b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We observe tails T. The previous posterior is the new prior.\n",
        "prior = posterior\n",
        "for i in range(3):\n",
        "    posterior[i] = likelihood_t[i] * prior[i]  / np.sum(likelihood_t * prior)\n",
        "print(posterior)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEY0jvpBv4Ch"
      },
      "source": [
        "The improbable first coin with heads and tails must be the truth (p=1), since the others were eliminated as impossible (p=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfsHZyhwLnx"
      },
      "source": [
        "### Monty Hall\n",
        "\n",
        "<img src=\"https://paulvanderlaken.files.wordpress.com/2020/04/1_znfmn84nnlhyx5tfdblzg.jpeg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2ApYm4zSbf"
      },
      "source": [
        "Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n",
        "\n",
        "https://en.wikipedia.org/wiki/Monty_Hall_problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOWgrRHF0EkZ"
      },
      "source": [
        "#### Counterintuitive\n",
        "\n",
        "Marilyn vos Savant gave the correct answer in her magazine column. Many PhDs \"chastised her for misleading and corrupting the\n",
        "public\". Some called her an idiot. But she was right !\n",
        "\n",
        "The renowned mathematician Paul Erdős was sceptical of the answer. He was reluctantly convinced with a computer simulation.\n",
        "\n",
        "https://sites.oxy.edu/lengyel/M372/Vazsonyi2003/vazs30_1.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lucr36F5nO7Q"
      },
      "source": [
        "### Answer\n",
        "\n",
        "Yes, it is to your advantage to switch your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsaJ8pBSncPA"
      },
      "source": [
        "### Simulation (repeated trials of a frequentist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLFCoGB15jNh"
      },
      "source": [
        "## Simulation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "no_change_strategy_wins = 0\n",
        "change_strategy_wins = 0\n",
        "\n",
        "all_doors = [1,2,3]\n",
        "\n",
        "for i in range(10000):\n",
        "  first_time_pick = np.random.choice(all_doors)\n",
        "  door_with_car = np.random.choice(all_doors)\n",
        "  all_doors_n = all_doors.copy()\n",
        "  if first_time_pick == door_with_car:\n",
        "    x = all_doors_n.remove(first_time_pick)\n",
        "    monty_opens_door = np.random.choice(all_doors_n)\n",
        "  else:\n",
        "    remain_doors = all_doors_n.remove(first_time_pick)\n",
        "    w = all_doors_n.remove(door_with_car)\n",
        "    monty_opens_door =  np.random.choice(all_doors_n)\n",
        "  second_time_pick = first_time_pick\n",
        "  if (second_time_pick == door_with_car):\n",
        "    no_change_strategy_wins = no_change_strategy_wins + 1\n",
        "  all_doors_n1 = all_doors.copy()\n",
        "  # Always change strategy\n",
        "  some_doors = all_doors_n1.remove(first_time_pick)\n",
        "  y = all_doors_n1.remove(monty_opens_door)\n",
        "  second_time_pick = all_doors_n1\n",
        "  if (second_time_pick == door_with_car):\n",
        "     change_strategy_wins=change_strategy_wins + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclp8nEeL9BE",
        "outputId": "75604f11-daab-4888-c8aa-d8140df2d1db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(change_strategy_wins/10000)\n",
        "print(no_change_strategy_wins/10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6608\n",
            "0.3392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZuZpftThlZ"
      },
      "source": [
        "It is to your advantage to switch your choice. The probability of winning is 2/3  if you switch and 1/3 if you stick to your first choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CM5YTpUUC0t"
      },
      "source": [
        "### Bayesian Solution\n",
        "\n",
        "Let us assume (without loss of generality) that you pick door 1 and the host Monty opens door 2.\n",
        "\n",
        "Let A be the event that the car is in door 1\n",
        "\n",
        "Let B be the event that the car is in door 2\n",
        "\n",
        "Let C be the event that the car is in door 3\n",
        "\n",
        "Let D be the event that Monty opens door 2\n",
        "\n",
        "According to Bayes, the probabilty of car in door 1 given Monty opens door 2 is:\n",
        "P(A|D) = P(D|A)*P(A) / P(D)\n",
        "\n",
        "Let us work on the denominator P(D) = P(D|A)*P(A) + P(D|B)P(B) + P(D|C)P(C)\n",
        "\n",
        "We got the algebra of Bayes rule, but what do these terms stand for.\n",
        "\n",
        "Let us begin with the numerator: P(D|A) means the probability that Monty opens door 2 given the car is in door 1. This is 1/2, since Monty can choose either of the remaining doors with the goats. Also, P(A) = 1/3, since the car can be in any door equally likely. So the numerator is 1/2 * 1/3 = 1/6\n",
        "\n",
        "Now the terms of the denominator:\n",
        "\n",
        "P(D|A)*P(A) is same as the numerator; it is 1/6\n",
        "\n",
        "P(D|B)*P(B) : First P(D|B) means the probability that Monty opens door 2 given the car is in door 2. That is impossible. Why would he show where the car is?\n",
        "So P(D|B) = 0 and P(D|B)*P(B) = 0*1/3 = 0\n",
        "\n",
        "P(D|C)P(C): First P(D|C) means the probability that Monty opens door 2 given the car is in door 3. That is certain, since he cannot open the door with the car (3). He is forced to open door 2 with a goat. So P(D|B ) = 1 and P(D|C)P(C) = 1*1/3 = 1/3.\n",
        "\n",
        "Now we have the values of all terms, so P(A|D) = 1/6  / (1/6 + 0 + 1/3) = 1/3\n",
        "\n",
        "A similar calculation shows that P(C|D) = 2/3 or you can see that P(C|D) = 1 - P(A|D) = 1 - 1/3 = 2/3\n",
        "\n",
        "Switching has P = 2/3 and sticking to first choice has p = 1/3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzB8WIMmBwhI"
      },
      "source": [
        "If you like the Monty Hall problem, here are similar ones with Bayesian solutions:\n",
        "http://allendowney.blogspot.com/2011/10/all-your-bayes-are-belong-to-us.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TugFdoeFnzQs"
      },
      "source": [
        "### Bayesian Machine Learning\n",
        "\n",
        "Let us consider the case of simple linear regression. We have values of X and Y (can be displayed on a scatterplot). We fit a linear regression: y = m*x + b\n",
        "\n",
        "Remember the slope m and intercept b are statistic of a sample. Classical statistics (frequentist) wants to infer the population parameters for slope and intercept. The Null hypothesis says that nothing is going on, i.e., slope = 0.\n",
        "We calculate a P-value and if it is less than a certain threshold, we reject the Null hypothesis.\n",
        "\n",
        "Bayesians agree with the frequentists that there is a fixed (but unknown) population parameter (slope) but they express their uncertainty in a probability distribution. They start with a prior probability distribution of the parameter and adjust it as soon as they get sample data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LzRndo5C0k7"
      },
      "source": [
        "### ScikitLearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaIsOMPYo2_v",
        "outputId": "3b05f3dc-79f3-4221-a0ca-922a9ec4646b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
        "Y = [0., 1., 2., 3.]\n",
        "reg = linear_model.BayesianRidge()\n",
        "reg.fit(X, Y)\n",
        "BayesianRidge()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n",
              "              compute_score=False, copy_X=True, fit_intercept=True,\n",
              "              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,\n",
              "              normalize=False, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZN464fQr-wF",
        "outputId": "69eced7c-5816-4946-a6b9-7c6b0c3f073e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reg.predict([[1, 0.]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.50000013])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTEVM9QZsGHD",
        "outputId": "a58f81d8-d4ea-4044-f756-9251d80e1401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reg.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.49999993, 0.49999993])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wIxwS5sNLQ"
      },
      "source": [
        "Due to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares. However, Bayesian Ridge Regression is more robust to ill-posed problems.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38FxXLbZsVWa"
      },
      "source": [
        "### Package PyMC3 (Probabilistic Programming in Python)\n",
        "\n",
        "This is a package for Bayesian statistical modeling and probabilistic machine learning\n",
        "\n",
        "An example of Bayesian linear regression with PyMC 3:\n",
        "https://docs.pymc.io/notebooks/GLM-linear.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d8znBbTvrQD"
      },
      "source": [
        "### Philosophy of Science\n",
        "\n",
        "Mathematics and Probability theory use deductive inference, i.e., from general statements to particular ones. If the axioms or premises are valid, then the conclusions are certain.\n",
        "\n",
        "Statistics, Physics, and other empirical sciences use inductive inference, i.e., from particular statements on observations (e.g., sample data) to general statements (e.g., on populations etc). But this is a leap of faith rather than logically justifiable. We are uncertain about conclusions in physics and statistics. This is called Hume's induction problem, after the 18th century Scottish philosopher David Hume ( I am a fan of him! )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5kwPnaxMRhu"
      },
      "source": [
        "\n",
        "<img src=\"https://media-exp1.licdn.com/dms/image/C4D12AQGB-CLRbvhf4A/article-cover_image-shrink_600_2000/0?e=1610582400&v=beta&t=nArvDx17hLr3H201vg1kd_8GF-AYXR1JN9u5ndabvrI\">\n",
        "\n",
        "The late Bertrand Russell explained it in an entertaining way, especially since **Thanksgiving** is coming soon:\n",
        "\n",
        "This turkey found that, on his first morning at the turkey farm, he was fed at 9 a.m. However, being a good inductivist, he did not jump to conclusions. He waited until he had collected a large number of observations of the fact that he was fed at 9 a.m., and he made these observations under a wide variety of circumstances, on Wednesdays and Thursdays, on warm days and cold days, on rainy days and dry days. Each day, he added another observation statement to his list. Finally, his inductivist conscience was satisfied and he carried out an inductive inference to conclude, “I am always fed at 9 a.m.”. Alas, this conclusion was shown to be false in no uncertain manner when, on Christmas eve, instead of being fed, he had his throat cut. An inductive inference with true premises has led to a false conclusion. (Alan Chalmers, What is this thing called Science, 2nd edition, University of Queensland Press, St. Lucia, 1982)\n"
      ]
    }
  ]
}
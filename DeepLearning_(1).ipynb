{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adinsa1/Data110/blob/main/DeepLearning_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e9eDC_L2-vl"
      },
      "source": [
        "### Deep Learning\n",
        "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.\n",
        "\n",
        "Yann LeCun, Yoshua Bengio & Geoffrey Hinton. Deep learning. Nature 521, 436–444 (28 May 2015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ubjSreY9Is_"
      },
      "source": [
        "### 2018 Turing Award\n",
        "\n",
        "<img src=\"https://awards.acm.org/binaries/content/gallery/acm/ctas/awards/turing-2018-bengio-hinton-lecun.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzIlCusq58Rm"
      },
      "source": [
        "### Artificial Neural Networks\n",
        "Deep learning is based on Artificial neural networks (ANNs) which are inspired by the biological neural networks in animal brains.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou14UKgk_B3P"
      },
      "source": [
        "### Traditional Machine Learning vs Deep Learning\n",
        "\n",
        "\n",
        "<img src=\"https://www.merkleinc.com/sites/default/files/inline-images/DL%20and%20ML1%20resized.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRigQKHDUot"
      },
      "source": [
        "### Gradient Descent\n",
        "\n",
        "The slope is the derivative of a single variable function f(x). It tells us the direction of x with the steepest ascent. The negative of the slope tells us the direction of x with the steepest descent.\n",
        "\n",
        "<img src=\"https://srdas.github.io/DLBook/DL_images/TNN1.png\">\n",
        "\n",
        "Gradient means the slope of a multivariable function with respect to each variable. It is a vector pointing in the direction of steepest ascent. The negative of the gradient points in the direction of steepest descent.\n",
        "\n",
        "Here is an analogy: You are hiking on a mountain. All of a sudden it got dark or foggy. You want to go to the bottom of the mountain (or valley). The negative of the gradient tells you the direction to take your step to get fastest to the bottom (steepest descent). Hence, the gradient descent.\n",
        "\n",
        "<img src=\"https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png\">\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/484/1*lIthvknHt9Tok5aIj4e__g.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LT_AO36Swup"
      },
      "source": [
        "### Learning Rate --Alpha\n",
        "\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE2-pmevA95e"
      },
      "source": [
        "### Logistic Regression as a Neural Network\n",
        "\n",
        "<img src=\"https://slideplayer.com/slide/12620833/76/images/8/Neuron+Model%3A+Logistic+Unit.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXnn-IpDy9I"
      },
      "source": [
        "### Backpropogation\n",
        "\n",
        "#### Naming Conventions: What we uses to call the coefficients (slopes with respect to each variable) in Linear Regression is now (in Deep Learning) called the weights. The intercept is now called the bias term.\n",
        "\n",
        "#### Chain Rule: You may remember composition of functions, e.g., h(x) = g(f(x)). To do the derivative of h(x), we use the chain rule. h'(x) = g'(x)*f'(x)\n",
        "\n",
        "For a single neuron network (e.g., Logistic Regression), it is  Sigmoid(Linear(x) + Bias) or sigmoid(θ0 + θ1x1 + θ2x2 + θ3x3 + θ4x4)\n",
        "\n",
        "The composition of functions gets longer with more neurons and layers.\n",
        "\n",
        "Backpropagation is an algorithm that computes the gradient of the cost (error) function with respect to each weight(coefficient or slope). It uses  the chain rule (Calculus) and propogates backwards from the output.\n",
        "\n",
        "<img src=\"https://images.deepai.org/glossary-terms/73eec54be08746f6b546a874580b8673/backpropagation.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpZrOxVap7dK"
      },
      "source": [
        "### Implemention of Single-layer Neural Network using NumPy\n",
        "\n",
        "The human brain is remarkable at learning new tasks and this is made possible by the neurons.\n",
        "Neurons learn through the process of trial and error, which we will be mimicking in this notebook.\n",
        "#### Task\n",
        "We will build a neural network that learns to predict 1 when a certain neuron is 1.\n",
        "\n",
        "**Train data**\n",
        "\n",
        "Input_1 | Input_2 | Input_3 | Output |\n",
        ":-------------: |:-------------: | :-------------: | :-------------: |\n",
        "0 | 0 | 0 | 0\n",
        "0 | 0 | 1 | 0\n",
        "0 | 1 | 0 | 1\n",
        "1 | 0 | 0 | 0\n",
        "1 | 1 | 0 | 1\n",
        "1 | 1 | 1 | 1\n",
        "\n",
        "#### Network Structure\n",
        "Our network has three inputs, three weights and one output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7VYH5ccp7dL"
      },
      "source": [
        "from numpy import exp, array, random, dot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLREXEj_p7dS"
      },
      "source": [
        "class SingleNeuronNetwork():\n",
        "    def __init__(self):\n",
        "        # We set the seed for the random number generator\n",
        "        # so that  same random numbers are reproduced\n",
        "        # every time the program is run\n",
        "        random.seed(42)\n",
        "\n",
        "        # --- Model a single neuron: 3 input connections and 1 output connection ---\n",
        "        # Assign random weights to a 3 x 1 matrix: Floating-point values in (-1, 1)\n",
        "        self.weights = 2 * random.random((3, 1)) - 1\n",
        "\n",
        "    # --- Define the Sigmoid function ---\n",
        "    # Pass the weighted sum of inputs through this function to normalize between [0, 1]\n",
        "    def __sigmoid(self, x):\n",
        "        return 1 / (1 + exp(-x))\n",
        "\n",
        "    # --- Define derivative of the Sigmoid function ---\n",
        "    # Evaluates confidence of existing learnt weights\n",
        "    def __sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # --- Define feed-forward procedure ---\n",
        "    def feed_forward(self, inputs):\n",
        "        # Feed-forward inputs through the single-neuron neural network\n",
        "        return self.__sigmoid(dot(inputs, self.weights))\n",
        "\n",
        "    # --- Define the training procedure ---\n",
        "    # Modufy weights by calculating error after every iteration\n",
        "    def train(self, train_inputs, train_outputs, num_iterations):\n",
        "        # We run the training for num_iteration times\n",
        "        for iteration in range(num_iterations):\n",
        "            # Feed-forward the training set through the single neuron neural network\n",
        "            output = self.feed_forward(train_inputs)\n",
        "\n",
        "            # Calculate the error in predicted output\n",
        "            # Difference between the desired output and the feed-forward output\n",
        "            error = train_outputs - output\n",
        "\n",
        "            # Multiply the error by the input and again by the gradient of Sigmoid curve\n",
        "            # 1. Less confident weights are adjusted more\n",
        "            # 2. Inputs, that are zero, do not cause changes to the weights\n",
        "            adjustment = dot(train_inputs.T, error *\n",
        "                             self.__sigmoid_derivative(output))\n",
        "\n",
        "            # Make adjustments to the weights\n",
        "            self.weights += adjustment\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSq7QBImp7dW"
      },
      "source": [
        "# Intialise a single-neuron neural network.\n",
        "neural_network = SingleNeuronNetwork()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzPFP72Tp7db",
        "outputId": "8e52f9ed-a47b-48e5-9ab0-0678041ddf16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print (\"Neural network weights before training (random initialization): \")\n",
        "print (neural_network.weights)\n",
        "\n",
        "# The train data consists of 6 examples, each consisting of 3 inputs and 1 output\n",
        "train_inputs = array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1,1,0], [1,1,1]])\n",
        "train_outputs = array([[0, 0, 1, 0, 1, 1]]).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural network weights before training (random initialization): \n",
            "[[-0.25091976]\n",
            " [ 0.90142861]\n",
            " [ 0.46398788]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em2sYBJHp7df",
        "outputId": "63e4421f-acbd-4819-9081-f974318b1eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Train the neural network using a train inputs.\n",
        "# Train the network for 10,000 steps while modifying weights to reduce error.\n",
        "neural_network.train(train_inputs, train_outputs, 10000)\n",
        "\n",
        "print (\"Neural network weights after training: \")\n",
        "print (neural_network.weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural network weights after training: \n",
            "[[-4.21652261]\n",
            " [12.79677774]\n",
            " [-4.21664048]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XzlX5thp7dj"
      },
      "source": [
        "**Test data**\n",
        "\n",
        "Now that we have trained the network, let us use the weights of the trained network to predict inputs that were not used to train the network:\n",
        "\n",
        "Input_1 | Input_2 | Input_3 | Expected Output |\n",
        ":-------------: |:-------------: | :-------------: | :-------------: |\n",
        "1 | 0 | 0 | 0\n",
        "0 | 1 | 1 | 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx6tAYLup7dk",
        "outputId": "3a703f3c-8e53-44e1-ab8b-e9fef86e8113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Test the neural network with a new input\n",
        "print (\"Inferring predicting from the network for [1, 0] -> ?: \")\n",
        "print (neural_network.feed_forward(array([1, 0, 0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inferring predicting from the network for [1, 0] -> ?: \n",
            "[0.01453545]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkH2a_2op7do",
        "outputId": "91cba362-17c4-45e8-d343-6c065f25dffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Inferring predicting from the network for [0, 1, 1] -> ?: \")\n",
        "print (neural_network.feed_forward(array([0, 1,1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inferring predicting from the network for [0, 1, 1] -> ?: \n",
            "[0.99981224]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSC9D6_QL45P"
      },
      "source": [
        "### Linear vs Non-Linear Classifiers\n",
        "\n",
        "<img src=\"https://assets.website-files.com/5cc74fa87e7513b0c52acc95/5cc7604a671396f4dab030a1_v7jVB_tJO9Vs8yc8dZRW4ZGrs2ujWhbjrJV_AMM_A1GT_GJMmhVVtT2nGM9fmZae_7e4kUzIzI-diTVVR2BxSdnEfO5LE_qNoMMJJj0Vc_BwwXbo4Ug8Qt5bm9nQqMNLm_NP8W2d.png\">\n",
        "\n",
        "Remember y = mx + b. The equation mx + b -y = 0 is a line that seperates the two classes. In higher dimensions (more features), a (hyper-)plane seperates the classes. Machine Learning notation: θ0 + θ1x1 + θ2x2 + θ3x3 + θ4x4 = 0 is the line or hyperplane that seperates the classes.\n",
        "\n",
        "<img src=\"https://jtsulliv.github.io/images/perceptron/linsep_new.png?raw=True\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXFVhT9WCNmJ"
      },
      "source": [
        "### Perceptron: XOR problem\n",
        "\n",
        "<img src=\"https://saedsayad.com/images/Perceptron_XOR.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlL77XEyygLv"
      },
      "source": [
        "### Deep Learning Frameworks\n",
        "\n",
        "Tensorflow is an open-source package for Neural Networks developed by Google. Keras is a high-level API that runs on top of Tensorflow. Easy to use for beginners.\n",
        "\n",
        "PyTorch is an open-source package for Neural Networks developed by Facebook.\n",
        "PyTorch is more popular in academia, whereas Tensorflow is more popular in industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ePGcTru0Ljr"
      },
      "source": [
        "### Keras/Tensorflow for the XOR problem:\n",
        "\n",
        "Here is a blog on how to implement XOR:\n",
        "\n",
        "https://blog.thoughtram.io/machine-learning/2016/11/02/understanding-XOR-with-keras-and-tensorlow.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fepdob_M0zll"
      },
      "source": [
        "### PyTorch for XOR\n",
        "\n",
        "https://courses.cs.washington.edu/courses/cse446/18wi/sections/section8/XOR-Pytorch.html\n"
      ]
    }
  ]
}